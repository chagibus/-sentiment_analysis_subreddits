{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e8c8513-331a-443d-8d94-09c3c0d715c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f2254c1b-0ab9-4c29-be97-a215a92afbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('./data/college_senti_zeroed.csv')\n",
    "opins = pd.read_csv('./data/opins_senti_oned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ca7bdc75-4100-4dd6-a207-981b7a837580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([opins, college], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3dd85ad0-8189-4351-bbfa-7f5206584983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.597545e+09</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>Middle aged guys don't buy sports cars because...</td>\n",
       "      <td>1646</td>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.613406e+09</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>\"Y'all\" is a brilliant use of the English lang...</td>\n",
       "      <td>885</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc         subreddit  \\\n",
       "0  1.597545e+09  unpopularopinion   \n",
       "1  1.613406e+09  unpopularopinion   \n",
       "\n",
       "                                                post  post_length  \\\n",
       "0  Middle aged guys don't buy sports cars because...         1646   \n",
       "1  \"Y'all\" is a brilliant use of the English lang...          885   \n",
       "\n",
       "   post_word_count  sent_score  \n",
       "0              310           1  \n",
       "1              179           1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4c9c9ae4-ac94-432b-9740-7bbc53d32f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 6)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f030d145-0343-4bf7-90eb-e6438477037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/combined_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eef99a44-a9b2-4963-8e96-58f0ea83562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/combined_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3520bb9e-e54f-4193-b520-91f58bc7b880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.597545e+09</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>Middle aged guys don't buy sports cars because...</td>\n",
       "      <td>1646</td>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.613406e+09</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>\"Y'all\" is a brilliant use of the English lang...</td>\n",
       "      <td>885</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc         subreddit  \\\n",
       "0  1.597545e+09  unpopularopinion   \n",
       "1  1.613406e+09  unpopularopinion   \n",
       "\n",
       "                                                post  post_length  \\\n",
       "0  Middle aged guys don't buy sports cars because...         1646   \n",
       "1  \"Y'all\" is a brilliant use of the English lang...          885   \n",
       "\n",
       "   post_word_count  sent_score  \n",
       "0              310           1  \n",
       "1              179           1  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "26e9abae-113d-4856-a56e-2b193ade96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['created_utc','post_length', 'post_word_count', 'subreddit'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4adb0b4-f885-4fd0-9ce0-06c604863f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Middle aged guys don't buy sports cars because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Y'all\" is a brilliant use of the English lang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  sent_score\n",
       "0  Middle aged guys don't buy sports cars because...           1\n",
       "1  \"Y'all\" is a brilliant use of the English lang...           1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21e47429-8b2a-46b3-ab32-ed2ad27a94bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f3f25-b8f8-44d2-b69e-cc8e7afc7593",
   "metadata": {},
   "source": [
    "# Preparing pooled data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4ec29f10-9019-420d-b16e-f64d1ee6fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text to lower case\n",
    "\n",
    "data['post'] = data['post'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "67880262-b72f-447d-98da-78c0ae2ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing chars and web addresses like https, www, .com from text\n",
    "import re\n",
    "def remove_chars(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c6ed25e4-02cb-41d8-9a92-84cf7a04d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing chars and web addresses like https, www, .com from text\n",
    "data['post_nochars'] = data['post'].apply(remove_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "257471b6-0acc-453b-a0db-b667416220d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i always eat dessert before dinner at a restaurantwhen at a restaurant, waiting staff always find it weird when i order the dessert before the appetizer and the main course. they ask “oh, is that all that you’re having?”. i’m like “no… i just want my cheesecake first, please”. i have to convince them that i’m just a dessert first kinda guy. i feel like the main course is much more enjoyable when your dopamine levels are boosted.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['post_nochars'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813ad1c-4043-4c3f-9dc9-0872a115ba7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9f643766-e517-4dab-b5de-d60d35ffcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text tokenization\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "data['post_tokenized'] = data['post_nochars'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b0f6a728-3b1a-4083-a65b-55cb4dac3a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['post_tokenized'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "67144904-e680-4125-a0c2-18b0ee2f34b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'always', 'eat', 'dessert', 'before', 'dinner', 'at', 'a', 'restaurantwhen', 'at', 'a', 'restaurant', 'waiting', 'staff', 'always', 'find', 'it', 'weird', 'when', 'i', 'order', 'the', 'dessert', 'before', 'the', 'appetizer', 'and', 'the', 'main', 'course', 'they', 'ask', 'oh', 'is', 'that', 'all', 'that', 'you', 're', 'having', 'i', 'm', 'like', 'no', 'i', 'just', 'want', 'my', 'cheesecake', 'first', 'please', 'i', 'have', 'to', 'convince', 'them', 'that', 'i', 'm', 'just', 'a', 'dessert', 'first', 'kinda', 'guy', 'i', 'feel', 'like', 'the', 'main', 'course', 'is', 'much', 'more', 'enjoyable', 'when', 'your', 'dopamine', 'levels', 'are', 'boosted']\n"
     ]
    }
   ],
   "source": [
    "print(data['post_tokenized'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bcfeee5b-d371-4b7f-9e8f-e1fb0bfbe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(words):\n",
    "    new_words = [token for token in words if token not in stopwords.words('english')]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "daae9cf3-9932-49b6-ac77-414ccce3960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['post_tokenized'] = data['post_tokenized'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb5fe596-e6c8-4733-b9f2-8a82a0aa44f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['middle',\n",
       " 'aged',\n",
       " 'guys',\n",
       " 'buy',\n",
       " 'sports',\n",
       " 'cars',\n",
       " 'mid',\n",
       " 'life',\n",
       " 'crisis',\n",
       " 'finally',\n",
       " 'afford',\n",
       " 'car',\n",
       " 'want',\n",
       " 'hate',\n",
       " 'hearing',\n",
       " 'people',\n",
       " 'say',\n",
       " 'oh',\n",
       " 'bought',\n",
       " 'corvette',\n",
       " 'balding',\n",
       " 'needs',\n",
       " 'feel',\n",
       " 'younger',\n",
       " 'someone',\n",
       " 'never',\n",
       " 'earned',\n",
       " 'much',\n",
       " 'money',\n",
       " 'made',\n",
       " 'spectacular',\n",
       " 'decision',\n",
       " 'love',\n",
       " 'rather',\n",
       " 'lucrative',\n",
       " 'finally',\n",
       " 'position',\n",
       " 'late',\n",
       " 'actually',\n",
       " 'save',\n",
       " 'buy',\n",
       " 'dream',\n",
       " 'car',\n",
       " 'get',\n",
       " 'cars',\n",
       " 'important',\n",
       " 'get',\n",
       " 'dislike',\n",
       " 'impact',\n",
       " 'environment',\n",
       " 'get',\n",
       " 'think',\n",
       " 'sports',\n",
       " 'cars',\n",
       " 'expensive',\n",
       " 'hassle',\n",
       " 'get',\n",
       " 'see',\n",
       " 'forty',\n",
       " 'something',\n",
       " 'guy',\n",
       " 'bmw',\n",
       " 'assume',\n",
       " 'compensating',\n",
       " 'something',\n",
       " 'realize',\n",
       " 'automotive',\n",
       " 'enthusiasm',\n",
       " 'huge',\n",
       " 'part',\n",
       " 'life',\n",
       " 'lot',\n",
       " 'people',\n",
       " 'often',\n",
       " 'biggest',\n",
       " 'connection',\n",
       " 'family',\n",
       " 'members',\n",
       " 'friends',\n",
       " 'car',\n",
       " 'enthusiast',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'increased',\n",
       " 'electrification',\n",
       " 'lower',\n",
       " 'carbon',\n",
       " 'footprint',\n",
       " 'ridiculous',\n",
       " 'torque',\n",
       " 'better',\n",
       " 'acceleration',\n",
       " 'accept',\n",
       " 'budget',\n",
       " 'increase',\n",
       " 'cool',\n",
       " 'exotic',\n",
       " 'car',\n",
       " 'decrease',\n",
       " 'pursuits',\n",
       " 'worth',\n",
       " 'like',\n",
       " 'might',\n",
       " 'enjoy',\n",
       " 'collecting',\n",
       " 'stamps',\n",
       " 'closet',\n",
       " 'full',\n",
       " 'nice',\n",
       " 'clothes',\n",
       " 'also',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'talk',\n",
       " 'call',\n",
       " 'dad',\n",
       " 'since',\n",
       " 'used',\n",
       " 'race',\n",
       " 'cars',\n",
       " 'motorcycles',\n",
       " 'bond',\n",
       " 'cars',\n",
       " 'car',\n",
       " 'news',\n",
       " 'next',\n",
       " 'time',\n",
       " 'see',\n",
       " 'guy',\n",
       " 'graying',\n",
       " 'temples',\n",
       " 'widow',\n",
       " 'peak',\n",
       " 'driving',\n",
       " 'ferrari',\n",
       " 'understand',\n",
       " 'might',\n",
       " 'realization',\n",
       " 'dream',\n",
       " 'childhood',\n",
       " 'try',\n",
       " 'know',\n",
       " 'crap',\n",
       " 'dream',\n",
       " 'dream']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['post_tokenized'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22da31-05a8-45da-a0b4-ad69cbcfa005",
   "metadata": {},
   "source": [
    "#### Text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "42272b54-ac98-46b9-96fd-6d42a1c0774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts-of-speech tagging \n",
    "\n",
    "#https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e2841451-9082-48a3-a62d-dc89ba224d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "def lemmatize_list(lwords):\n",
    "    tags = nltk.pos_tag(lwords)\n",
    "    tagged = [(word, get_wordnet_pos(tag)) for (word, tag) in tags]\n",
    "    lemmatized_words = [wn.lemmatize(word, tag) if tag != '' else word for (word, tag) in tagged]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5503033b-67d7-482a-ac7d-3b2e24296f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing the text\n",
    "data['post_lemmatized'] = data['post_tokenized'].apply(lemmatize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9f751b8c-e33e-4155-875a-c10a401e7959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [middle, age, guy, buy, sport, car, mid, life,...\n",
       "1       [brilliant, use, english, language, refuse, te...\n",
       "2       [always, eat, dessert, dinner, restaurantwhen,...\n",
       "3       [illegal, company, list, entry, level, job, re...\n",
       "4       [news, dry, recitation, fact, opinion, intend,...\n",
       "                              ...                        \n",
       "3579    [rough, st, semesteri, finish, first, semester...\n",
       "3580    [many, flashcard, make, study, exam, take, try...\n",
       "3581    [grade, receive, first, ever, uni, college, es...\n",
       "3582    [mental, health, kill, semester, campus, resou...\n",
       "3583    [mess, please, help, maintain, attendance, int...\n",
       "Name: post_lemmatized, Length: 3584, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['post_lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d0556a32-c92b-4275-ad3f-447f0d2558c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>post_nochars</th>\n",
       "      <th>post_tokenized</th>\n",
       "      <th>post_lemmatized</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>middle aged guys don't buy sports cars because...</td>\n",
       "      <td>1</td>\n",
       "      <td>middle aged guys don't buy sports cars because...</td>\n",
       "      <td>[middle, aged, guys, buy, sports, cars, mid, l...</td>\n",
       "      <td>[middle, age, guy, buy, sport, car, mid, life,...</td>\n",
       "      <td>middle age guy buy sport car mid life crisis f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"y'all\" is a brilliant use of the english lang...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"y'all\" is a brilliant use of the english lang...</td>\n",
       "      <td>[brilliant, use, english, language, refuse, to...</td>\n",
       "      <td>[brilliant, use, english, language, refuse, te...</td>\n",
       "      <td>brilliant use english language refuse tell oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i always eat dessert before dinner at a restau...</td>\n",
       "      <td>1</td>\n",
       "      <td>i always eat dessert before dinner at a restau...</td>\n",
       "      <td>[always, eat, dessert, dinner, restaurantwhen,...</td>\n",
       "      <td>[always, eat, dessert, dinner, restaurantwhen,...</td>\n",
       "      <td>always eat dessert dinner restaurantwhen resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it should be illegal for a company to list an ...</td>\n",
       "      <td>1</td>\n",
       "      <td>it should be illegal for a company to list an ...</td>\n",
       "      <td>[illegal, company, list, entry, level, job, re...</td>\n",
       "      <td>[illegal, company, list, entry, level, job, re...</td>\n",
       "      <td>illegal company list entry level job require y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news should be a dry recitation of facts. if i...</td>\n",
       "      <td>1</td>\n",
       "      <td>news should be a dry recitation of facts. if i...</td>\n",
       "      <td>[news, dry, recitation, facts, opinion, intend...</td>\n",
       "      <td>[news, dry, recitation, fact, opinion, intend,...</td>\n",
       "      <td>news dry recitation fact opinion intend stir e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  sent_score  \\\n",
       "0  middle aged guys don't buy sports cars because...           1   \n",
       "1  \"y'all\" is a brilliant use of the english lang...           1   \n",
       "2  i always eat dessert before dinner at a restau...           1   \n",
       "3  it should be illegal for a company to list an ...           1   \n",
       "4  news should be a dry recitation of facts. if i...           1   \n",
       "\n",
       "                                        post_nochars  \\\n",
       "0  middle aged guys don't buy sports cars because...   \n",
       "1  \"y'all\" is a brilliant use of the english lang...   \n",
       "2  i always eat dessert before dinner at a restau...   \n",
       "3  it should be illegal for a company to list an ...   \n",
       "4  news should be a dry recitation of facts. if i...   \n",
       "\n",
       "                                      post_tokenized  \\\n",
       "0  [middle, aged, guys, buy, sports, cars, mid, l...   \n",
       "1  [brilliant, use, english, language, refuse, to...   \n",
       "2  [always, eat, dessert, dinner, restaurantwhen,...   \n",
       "3  [illegal, company, list, entry, level, job, re...   \n",
       "4  [news, dry, recitation, facts, opinion, intend...   \n",
       "\n",
       "                                     post_lemmatized  \\\n",
       "0  [middle, age, guy, buy, sport, car, mid, life,...   \n",
       "1  [brilliant, use, english, language, refuse, te...   \n",
       "2  [always, eat, dessert, dinner, restaurantwhen,...   \n",
       "3  [illegal, company, list, entry, level, job, re...   \n",
       "4  [news, dry, recitation, fact, opinion, intend,...   \n",
       "\n",
       "                                        joined_words  \n",
       "0  middle age guy buy sport car mid life crisis f...  \n",
       "1  brilliant use english language refuse tell oth...  \n",
       "2  always eat dessert dinner restaurantwhen resta...  \n",
       "3  illegal company list entry level job require y...  \n",
       "4  news dry recitation fact opinion intend stir e...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['joined_words'] = data[\"post_lemmatized\"].map(' '.join)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a6480-a916-4374-b9ac-eff651eacacb",
   "metadata": {},
   "source": [
    "#### Save lemmatized pooled posts as .CSV file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dced0c21-438d-4ff0-9b19-3858a1ccccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/lemmatized_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "941ccae8-0940-4cce-bc1c-3a02ac327bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['joined_words']\n",
    "y = data['sent_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1d93c27b-5281-4106-9f53-88209d46f82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent_score\n",
       "1    0.53404\n",
       "0    0.46596\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755f701-10ad-4328-867e-4eca0eb0d1fd",
   "metadata": {},
   "source": [
    "### Preparing college posts for predictions from various trained ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "181de1c8-af9a-48d0-941b-7a7bddfdc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('./data/college_senti_zeroed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3412d1fc-e5a5-4397-8d0a-0f13d84f0e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.543598e+09</td>\n",
       "      <td>college</td>\n",
       "      <td>Ever have a kid in class show up and realize i...</td>\n",
       "      <td>871</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc subreddit                                               post  \\\n",
       "0  1.543598e+09   college  Ever have a kid in class show up and realize i...   \n",
       "\n",
       "   post_length  post_word_count  sent_score  \n",
       "0          871              174           0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "17789035-8449-4665-81a6-73c1a0ba3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "college.drop(columns = ['created_utc', 'subreddit', 'post_length', 'post_word_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b8d6cbeb-9ec0-407e-9fe9-9e692497811c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ever have a kid in class show up and realize i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It’s the little things that count, this profes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Professor saved my assDuring winter term at my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My professor gave out all the answers to the f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An Honest Letter from Your University Presiden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  sent_score\n",
       "0  Ever have a kid in class show up and realize i...           0\n",
       "1  It’s the little things that count, this profes...           0\n",
       "2  Professor saved my assDuring winter term at my...           0\n",
       "3  My professor gave out all the answers to the f...           0\n",
       "4  An Honest Letter from Your University Presiden...           0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7192c77f-46f5-48c7-9382-3b5dc4ceff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "college['post'] = college['post'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "419283fd-d432-4f47-a6f0-a1970d769230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_chars(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "55df7f00-5f16-49d4-b35b-979396fcf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "college['post_tokenized'] = college['post'].apply(remove_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "61dc687e-75e2-4208-9995-d444272a55a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "921"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(college['post_tokenized'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d134c6b7-710e-4dcf-9c98-2163f34aa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "college['post_tokenized'] = college['post_tokenized'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bf7b8f21-5ad8-4ce7-ab83-b33b8ae4351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(college['post_tokenized'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "72c3a662-77ad-487d-b050-ed34d97c1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'professor', 'gave', 'out', 'all', 'the', 'answers', 'to', 'the', 'final', 'i', 'just', 'left', 'the', 'exam', 'hall', 'for', 'my', 'cellular', 'and', 'molecular', 'biology', 'class', 'gen', 'bio', 'and', 'i', 'am', 'absolutely', 'baffled', 'right', 'now', 'last', 'week', 'my', 'professor', 'gave', 'us', 'a', 'packet', 'with', 'questions', 'and', 'also', 'gave', 'the', 'answers', 'she', 'said', 'they', 'would', 'be', 'a', 'similar', 'style', 'to', 'the', 'final', 'and', 'give', 'us', 'an', 'idea', 'of', 'what', 'topics', 'to', 'study', 'most', 'i', 'just', 'took', 'the', 'exam', 'and', 'it', 'was', 'literally', 'the', 'review', 'packet', 'question', 'for', 'question', 'she', 'even', 'reprinted', 'it', 'with', 'the', 'word', 'review', 'obviously', 'crossed', 'out', 'so', 'it', 'just', 'read', 'final', 'exam', 'needless', 'to', 'say', 'i', 'finished', 'it', 'in', 'minutes', 'as', 'did', 'half', 'of', 'the', 'class', 'i', 'genuinely', 'think', 'she', 'intended', 'to', 'write', 'a', 'new', 'final', 'but', 'she', 'realized', 'how', 'behind', 'she', 'is', 'on', 'grading', 'hasn', 't', 'graded', 'our', 'lab', 'practical', 'final', 'or', 'any', 'lab', 'reports', 'plus', 'she', 'has', 'the', 'flu', 'so', 'she', 'just', 'said', 'fuck', 'it', 'she', 'has', 'been', 'so', 'lazy', 'and', 'disorganized', 'all', 'semester', 'this', 'class', 'has', 'been', 'the', 'easiest', 'a', 'of', 'my', 'life']\n"
     ]
    }
   ],
   "source": [
    "print(college['post_tokenized'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5a65057b-891a-4863-be1f-2fd0aef41659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    new_words = [token for token in words if token not in stopwords.words('english')]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3201d19c-b08d-4055-a0d2-858280fa420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "college['post_tokenized'] = college['post_tokenized'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0e567d0-e9e0-4676-917c-04528d040bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0008ecf9-8cc5-40db-93ab-a7966f5f3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(lwords):\n",
    "    tags = nltk.pos_tag(lwords)\n",
    "    tagged = [(word, get_wordnet_pos(tag)) for (word, tag) in tags]\n",
    "    lemmatized_words = [wn.lemmatize(word, tag) if tag != '' else word for (word, tag) in tagged]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "20a253f2-9e9d-4294-b1ef-777d562783b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "college['post_lemmatized'] = college['post_tokenized'].apply(lemmatize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f20e7d29-4f8c-4eb3-b6d6-ab59749beef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>post_tokenized</th>\n",
       "      <th>post_lemmatized</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ever have a kid in class show up and realize i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ever, kid, class, show, realize, exam, day, g...</td>\n",
       "      <td>[ever, kid, class, show, realize, exam, day, g...</td>\n",
       "      <td>ever kid class show realize exam day get leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it’s the little things that count, this profes...</td>\n",
       "      <td>0</td>\n",
       "      <td>[little, things, count, professor, gem, stayed...</td>\n",
       "      <td>[little, thing, count, professor, gem, stay, l...</td>\n",
       "      <td>little thing count professor gem stay late com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professor saved my assduring winter term at my...</td>\n",
       "      <td>0</td>\n",
       "      <td>[professor, saved, assduring, winter, term, un...</td>\n",
       "      <td>[professor, save, assduring, winter, term, uni...</td>\n",
       "      <td>professor save assduring winter term uni bad m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my professor gave out all the answers to the f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[professor, gave, answers, final, left, exam, ...</td>\n",
       "      <td>[professor, give, answer, final, leave, exam, ...</td>\n",
       "      <td>professor give answer final leave exam hall ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an honest letter from your university presiden...</td>\n",
       "      <td>0</td>\n",
       "      <td>[honest, letter, university, president, openin...</td>\n",
       "      <td>[honest, letter, university, president, open, ...</td>\n",
       "      <td>honest letter university president open fallde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  sent_score  \\\n",
       "0  ever have a kid in class show up and realize i...           0   \n",
       "1  it’s the little things that count, this profes...           0   \n",
       "2  professor saved my assduring winter term at my...           0   \n",
       "3  my professor gave out all the answers to the f...           0   \n",
       "4  an honest letter from your university presiden...           0   \n",
       "\n",
       "                                      post_tokenized  \\\n",
       "0  [ever, kid, class, show, realize, exam, day, g...   \n",
       "1  [little, things, count, professor, gem, stayed...   \n",
       "2  [professor, saved, assduring, winter, term, un...   \n",
       "3  [professor, gave, answers, final, left, exam, ...   \n",
       "4  [honest, letter, university, president, openin...   \n",
       "\n",
       "                                     post_lemmatized  \\\n",
       "0  [ever, kid, class, show, realize, exam, day, g...   \n",
       "1  [little, thing, count, professor, gem, stay, l...   \n",
       "2  [professor, save, assduring, winter, term, uni...   \n",
       "3  [professor, give, answer, final, leave, exam, ...   \n",
       "4  [honest, letter, university, president, open, ...   \n",
       "\n",
       "                                        joined_words  \n",
       "0  ever kid class show realize exam day get leave...  \n",
       "1  little thing count professor gem stay late com...  \n",
       "2  professor save assduring winter term uni bad m...  \n",
       "3  professor give answer final leave exam hall ce...  \n",
       "4  honest letter university president open fallde...  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college['joined_words'] = college[\"post_lemmatized\"].map(' '.join)\n",
    "college.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202fcf9-2e7d-46cd-a38f-5c9be6c77e5e",
   "metadata": {},
   "source": [
    "#### Save lemmatized college posts as .CSV file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29dcf77d-d97e-4710-ad6c-e768688d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "college.to_csv('./data/coll_lemmatized_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b099fb4-1b6c-4798-8997-638cabddd9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4518642-f4fa-4911-a2f1-f6842eda242c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
